# app.py

import torch
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import AutoPeftModelForCausalLM

# === –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ —Å chat_template ===
model_path = "nikatonika/Llama-3.2-1B-Instruct_v1_ext_chat_template"

# === –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ ===
tokenizer = AutoTokenizer.from_pretrained(model_path)
tokenizer.pad_token = tokenizer.eos_token

model = AutoPeftModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto"
)
model.eval()

# === –ì–µ–Ω–µ—Ä–∞—Ü–∏—è ===
async def generate_response(user_input, max_new_tokens, temperature, top_p):
    # –ü—Ä–∏–º–µ—Ä –≤–Ω–µ—à–Ω–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–∞—Ü–∏–µ–Ω—Ç–∞)
    context = "Patient is female, 43, reports fatigue and high stress."

    prompt = f"""{context}

You are Dr. Gregory House, a world-class diagnostician known for sarcasm, wit, and medical expertise.
You don't sugarcoat anything and always rely on logic and medical facts.

Answer concisely, with dry humor and intelligence.

User: {user_input}
Dr. House:"""

    inputs = tokenizer(prompt.strip(), return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=True,
            num_return_sequences=3,  # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
            pad_token_id=tokenizer.eos_token_id
        )

    responses = [tokenizer.decode(o, skip_special_tokens=True).split("Dr. House:")[-1].strip() for o in outputs]

    # –ü—Ä–æ—Å—Ç–æ–π —Ä–∞–Ω–∂–∏—Ä–æ–≤—â–∏–∫: –≤—ã–±—Ä–∞—Ç—å —Å–∞–º—ã–π –∫–æ—Ä–æ—Ç–∫–∏–π –∞–¥–µ–∫–≤–∞—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É —Ç–æ—á–∫—É
    best = sorted(responses, key=lambda r: (len(r.split()), -r.count(".")))[0]

    # –û–±—Ä–µ–∑–∫–∞ –ø–æ –≤—Ç–æ—Ä–æ–π —Ç–æ—á–∫–µ
    if best.count(".") >= 2:
        best = ".".join(best.split(".")[:2]) + "."

    return best

# === Gradio-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å ===
with gr.Blocks() as demo:
    gr.Markdown("## üß† HouseMD Chatbot ‚Äî Llama-3.2-1B ChatTemplate")
    gr.Markdown("–î–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –∏–º–∏—Ç–∏—Ä—É—é—â–∞—è —Å—Ç–∏–ª—å –¥–æ–∫—Ç–æ—Ä–∞ –•–∞—É—Å–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è `chat_template` –∏ —Ä—É—á–Ω–æ–π prompt —Å —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º.")

    with gr.Row():
        with gr.Column(scale=2):
            user_input = gr.Textbox(label="–í–æ–ø—Ä–æ—Å –ø–∞—Ü–∏–µ–Ω—Ç–∫–∏", placeholder="Why am I still sick?")
            max_tokens = gr.Slider(32, 200, value=80, step=8, label="–ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤")
            temperature = gr.Slider(0.1, 1.5, value=0.7, step=0.1, label="Temperature")
            top_p = gr.Slider(0.5, 1.0, value=0.9, step=0.05, label="Top-p")
            button = gr.Button("–°–ø—Ä–æ—Å–∏—Ç—å –¥–æ–∫—Ç–æ—Ä–∞ –•–∞—É—Å–∞")
        with gr.Column(scale=3):
            output = gr.Textbox(label="–û—Ç–≤–µ—Ç –•–∞—É—Å–∞", lines=4)

    button.click(fn=generate_response,
                 inputs=[user_input, max_tokens, temperature, top_p],
                 outputs=output)

if __name__ == "__main__":
    demo.launch()